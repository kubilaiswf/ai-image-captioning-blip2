# OBSS AI Image Captioning Challenge - My Solution

This repository contains my submission and approach for the OBSS AI Image Captioning Challenge, part of the OBSS Deep Learning Internship Program (2025). The goal of the challenge was to generate descriptive captions for a diverse set of images.

## Overview

My solution utilizes the **Salesforce BLIP-2 OPT 2.7B model**, a powerful pre-trained vision-language model, for generating image captions. The approach focuses on zero-shot inference, meaning the model was used without any additional fine-tuning on the competition's training data. Key considerations were made for resource efficiency to run within the Google Colab Free Tier environment.

## Tech Stack

*   **Programming Language:** Python 3.11
*   **Core Libraries:**
    *   PyTorch (version specified in notebook)
    *   HuggingFace Transformers (version specified in notebook)
    *   Pillow (PIL)
    *   Pandas
    *   Tqdm
*   **Model & Techniques:**
    *   **Model:** `Salesforce/blip2-opt-2.7b`
    *   **Processor:** `Blip2Processor`
    *   **Quantization:** 8-bit (via `BitsAndBytesConfig`)
    *   **Precision:** `torch.float16` for inference
*   **Environment:** Google Colab (Free Tier - NVIDIA T4 GPU)

## Approach

1.  **Model Loading & Optimization:**
    *   The BLIP-2 OPT 2.7B model was loaded from HuggingFace.
    *   To fit within the 15GB VRAM of the T4 GPU, 8-bit quantization was applied using `BitsAndBytesConfig`.
    *   Inference was performed using `torch.float16` precision.
    *   `device_map="auto"` was used for optimal layer distribution.

2.  **Image Processing & Caption Generation:**
    *   Test images were loaded using Pillow and converted to RGB.
    *   The `Blip2Processor` was used to preprocess images and tokenize a standard text prompt.
    *   **Prompt:** A consistent prompt, `"a photo of"`, was used for all images to guide the caption generation.
    *   **Inference:** The model generated captions with `max_new_tokens=50` to control length.
    *   **Post-processing:** Generated token IDs were decoded back to text, and special tokens were skipped.

3.  **No Fine-tuning:**
    *   Due to time and resource constraints, and to leverage the strong zero-shot capabilities of BLIP-2, no fine-tuning was performed on the provided training data.

## How to Run

1.  **Setup:**
    *   Ensure you have a Python environment (Google Colab is recommended for GPU access).
    *   Install the required libraries:
        ```bash
        pip install torch torchvision torchaudio
        pip install transformers accelerate timm bitsandbytes pillow pandas tqdm
        ```
2.  **Data:**
    *   Download the competition dataset ("obss-intern-competition-2025.zip") from the Kaggle competition page.
    *   Unzip the data into a directory (e.g., `obss_data/`). The notebook expects the test images in `obss_data/test/test/` and `test.csv` in `obss_data/`.
3.  **Notebook:**
    *   Open and run the `submission.ipynb` notebook.
    *   The notebook will:
        *   Load the pre-trained BLIP-2 model and processor.
        *   Iterate through the test images.
        *   Generate captions for each image.
        *   Save the results to `submission.csv`.

## Results & Observations

*(This section can be a brief summary of what you discussed in your report's "Analysis of Model Performance" section. You can mention where it performed well and where it struggled.)*

The model demonstrated good performance on images with clear subjects and common scenes. For instance, it accurately captioned images of laptops in stores, airplanes on tarmacs, and specific branded products.

Challenges were observed with:
*   Complex scenes with multiple salient objects, where the model might oversimplify or focus on a single element.
*   Generating highly specific captions for detailed images, sometimes resulting in overly generic descriptions.
*   Occasional repetitive token generation on particularly ambiguous visual inputs.

The zero-shot approach provided a strong baseline, but fine-tuning or more advanced prompt engineering could potentially address some of these limitations.

## Files in this Repository

*   `submission.ipynb`: The Jupyter Notebook containing all the code for loading the model, processing images, generating captions, and creating the submission file.
*   `report.pdf`: The detailed participant report submitted for the challenge.
*   `submission.csv`: An example output CSV file generated by the notebook for the test set. *(You can choose to include your actual submission.csv or just mention it's generated by the notebook)*
*   `README.md`: This file.

## Acknowledgements

*   Salesforce for the BLIP-2 model.
*   HuggingFace for the Transformers library and model hosting.

---
